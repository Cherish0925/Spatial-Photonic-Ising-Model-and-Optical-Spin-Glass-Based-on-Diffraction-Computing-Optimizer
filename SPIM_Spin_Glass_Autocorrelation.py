# First, define the solution scale of this Ising model, which is the number of optimized spin states.
# Then, the coupling interaction matrix between spins in the Ising problem is introduced.
# Then, using the general matrix decomposition method,
# the high-rank coupling matrix is decomposed into a series of rank-1 vector outer products,
# and normalization is performed to obtain the weight of each rank-1 vector.
# Using the tensor operations provided by Pytorch,
# the spatial optical Ising machine process corresponding to each rank-1 vector is rapidly solved.
# The Hamiltonian of each sub-vector is weighted and summed.
# Combining this process with the annealing algorithm,
# a general solution method for optimizing the QUBO problem using the spatial optical Ising machine is formed.

"""
    # The simple operations are accomplished based on the CPU and Numpy,
    # while the complex parallel computations are handled by the GPU and Pytorch.
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from decompose_J_coupling_matrix import decompose_ising_coupling_with_rank, ising_2d_coupling, ising_coupling_rect, \
    decompose_ising_coupling_with_rank_visualize
from Bandlimited_ASM import _center_embed, _center_crop, BandlimitedASM
from vec2dpm import pack_vectors_to_square, dph_encode_real_single_slm, build_input_amplitude_columnwise, \
    spins_vector_to_block_image
import math
from typing import Union, Tuple, Optional
from get_hamiltonian import hamiltonian_from_Eout
from datas_process import spins01_to_pm1, mean_magnetization_pm1, spin_autocorr, SixPanelInputs, plot_six_panel, \
    spin_autocorr_general
from scipy.optimize import curve_fit

width, height = 10, 10
# case 4 æ¨¡æ‹Ÿè‡ªæ—‹ç»ç’ƒã€é«˜æ–¯åˆ†å¸ƒçš„è€¦åˆå¼ºåº¦çŸ©é˜µ
a = 2 * np.random.rand(width * height, width * height) - 1
J_Coupling = a + a.T
print(rf"The spin-coupling interaction matrix has been obtained.")
print(rf"J_Coupling.shape={J_Coupling.shape}, J_Coupling.dtype={J_Coupling.dtype}")
spins_total = J_Coupling.shape[0]  # or J_Coupling.shape[1]
n = int(np.sqrt(spins_total))
if np.power(n, 2) < spins_total:
    n = n + 1
print(rf"Create a spin matrix of size {n}*{n}, but only {spins_total} spins are filled in it.")
alpha = 1.0
iterations = 10000
Method = "Simulated_Annealing"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(rf"Computing device is {device}")
wavelength = 634e-9  # wavelength [m]
pixel_size = 8e-6  # pixel size [m]
lens_focal_length = 100e-3
dtype = torch.complex64
# å•è‡ªæ—‹è€¦åˆå› å­çš„æ‰©å¢å€æ•°ï¼Œå®é™…ä¸Šæ˜¯1*1ä¸ªåƒç´ å˜æˆäº†16*16ä¸ª;ç»éªŒå€¼è‚¯å®šæ˜¯è¶Šå¤§è¶Šå¥½ï¼›2å’Œ4éƒ½è¯•è¿‡äº†æ²¡æœ‰8å¥½ï¼›16æœ€å¥½ï¼Œç›®å‰æ¥çœ‹ï¼›12çš„æ•ˆæœä¹Ÿè¿˜å¯ä»¥
pixel_repeat_Coupling_Jij = 8  # The amplification factor of the single-spin coupling constant actually means that 1*1 pixel has been transformed into 16*16 pixels.
collector = 6  # 6 åŸæœ¬å°±å¯ä»¥äº†ï¼›æœ€ç»ˆè¾“å‡ºå…‰å¼ºé¢çš„èšç„¦çš„åŒºåŸŸã€‚æ”¶é›†ä¸­é—´ 6*6 åƒç´ åŒºåŸŸä¸­çš„å…‰å¼ºçš„å’Œ
# T_list = torch.logspace(np.log10(0.2), np.log10(5.0), 12)  # 0.2 â€¦ 5.0
T_list = torch.tensor([0.001, 0.010, 0.10, 0.3,0.6, 1.0,1.25,1.5, 3.0, 10.0])
C_dict = {}
qEA_arr = []
tau_arr = []

vectors, weights, r, order, J_rec, mse_map = decompose_ising_coupling_with_rank_visualize(
    J_Coupling,
    tol=1e-10,
    return_full=False,  # åªè¦å‰ r åˆ—
    visualize=True,
    visualize_mode="live",  # é‡ç‚¹ï¼šéé˜»å¡æ˜¾ç¤º
    pause_s=0.2,  # ç»™ GUI ä¸€ç‚¹æ—¶é—´åˆ·æ–°
    fig_title=f"Decomposition for {J_Coupling.shape[0]} spins",
    extras_out=True,  # éœ€è¦ J_rec / mse_map
    fig_tag="J-decomp@case1"  # çª—å£æ ‡é¢˜ï¼›å¤šæ¬¡è°ƒç”¨æ—¶å¯æ¢ä¸€ä¸ª tag
)
print(rf"J_rec.shape={J_rec.shape}, J_rec.dtype={J_rec.dtype}")
print(rf"mse_map.shape={mse_map.shape}, mse_map.dtype={mse_map.dtype}")
print(
    rf"The spin-coupling interaction matrix has been decomposed into {r} matrices of rank 1, all of which are of equal size.")
print(rf"The dimension of the decomposed vector matrix with a rank of 1 is {vectors.shape}")
print(rf"The corresponding number of weight vectors is {weights.shape}")

# Next, based on the dual-phase encoding method,
# r coupling coefficient vectors are encoded onto the complex amplitude of the spatial light.
# Each DMP-Spin pixel is 8*8()
real = torch.randn(100, 160, 160, device=device)
imag = torch.randn(100, 160, 160, device=device)
Ein = torch.complex(real, imag).to("cuda")  # è‡ªåŠ¨ dtype=torch.complex64

Square_tensor = pack_vectors_to_square(vectors, return_torch=True, device="cpu", dtype=torch.float32,
                                       pixel_repeat=pixel_repeat_Coupling_Jij)
print(rf"Square_tensor.shape={Square_tensor.shape},Square_tensor.dtype={Square_tensor.dtype}")
print(rf"torch.max(Square_tensor)={torch.max(Square_tensor)}, torch.min(Square_tensor)={torch.min(Square_tensor)}")
dpm_phase, B = dph_encode_real_single_slm(Square_tensor, tile=1, to01=True, clamp=True)
print(rf"dpm_phase.shape={dpm_phase.shape},dpm_phase.dtype={dpm_phase.dtype}")
print(rf"torch.max(dpm_phase)={torch.max(dpm_phase)}, torch.min(dpm_phase)={torch.min(dpm_phase)}")
asm = BandlimitedASM(wavelength=wavelength, N_size=int(n * pixel_repeat_Coupling_Jij), pixel_size=pixel_size,
                     zero_fill=1, is_lens=True, distance=lens_focal_length,
                     sampling="midpoint", dtype=dtype, device=device, verbose=True)
# ä¸‹é¢å®šä¹‰è¾“å…¥çš„æŒ¯å¹…çŸ©é˜µï¼ˆæŸäº›ç†åº”ç©ºç™½çš„ä½ç½®ä¸åº”è¯¥æœ‰å•ä½å…‰åœºè¾“å…¥ï¼Œå¦åˆ™ç›¸å½“äºå¢åŠ äº†ç­‰æ•ˆçš„å¤–åŠ ç£åœºï¼‰ä»¥åŠåˆå§‹è‡ªæ—‹æ ¼ç‚¹ï¼ˆä¸€åˆ—å‘é‡ã€æˆ–è€…æ‹†åˆ†ä¸€åˆ—ä¸€åˆ—ä¸ºçŸ©é˜µ
# ç„¶åæ¯è½®ä¼˜åŒ–è¿­ä»£å¼€å§‹æ—¶éƒ½æ‹¼æ¥ä¸º torch.complex64
input_amp = build_input_amplitude_columnwise(spins_total=spins_total, pixel_repeat=pixel_repeat_Coupling_Jij,
                                             device=device, return_batch=False)
print(rf"input_amp.shape={input_amp.shape}, input_amp.dtype={input_amp.dtype}, input_amp.device={input_amp.device}")

initial_spins_state = torch.randint(0, 2, (spins_total,))

init_spin_phase = spins_vector_to_block_image(initial_spins_state,
                                              pixel_repeat=1,
                                              return_batch=False)  # [1,H,W]

print(
    rf"initial_spins_state.shape={initial_spins_state.shape}, initial_spins_state.dtype={initial_spins_state.dtype}, initial_spins_state.device={initial_spins_state.device}")
input_spin_phase = spins_vector_to_block_image(initial_spins_state,
                                               pixel_repeat=pixel_repeat_Coupling_Jij,
                                               return_batch=False).cuda()  # [1,H,W]
print(
    rf"input_spin_phase.shape={input_spin_phase.shape}, input_spin_phase.dtype={input_spin_phase.dtype}, input_spin_phase.device={input_spin_phase.device}")
# --- ç¡®ä¿ dpm_phase æœ‰ batch ç»´ [r,H,W] ---
# ä½ è¿™æ¬¡åˆ†è§£åªæœ‰ä¸€ä¸ªç§© r=1ï¼Œdph_encode_real_single_slm è¿”å›äº† [H,W] è¢«ä½ ç›´æ¥ç”¨äº†
if dpm_phase.ndim == 2:
    dpm_phase = dpm_phase.unsqueeze(0)  # -> [1,H,W]
dpm_phase_exp = torch.exp(-1j * 2 * torch.pi * dpm_phase).cuda()  # coupling matrix
print(
    rf"dpm_phase_exp.shape={dpm_phase_exp.shape}, dpm_phase_exp.dtype={dpm_phase_exp.dtype}, dpm_phase_exp.device={dpm_phase_exp.device}")
input_amp_with_dpm_phase_exp = dpm_phase_exp * input_amp
print(
    rf"input_amp_with_dpm_phase_exp.shape={input_amp_with_dpm_phase_exp.shape}, input_amp_with_dpm_phase_exp.dtype={input_amp_with_dpm_phase_exp.dtype}, input_amp_with_dpm_phase_exp.device={input_amp_with_dpm_phase_exp.device}")

E_in = input_amp_with_dpm_phase_exp * torch.exp(-1j * torch.pi * input_spin_phase)
print(rf"E_in.shape={E_in.shape}, E_in.dtype={E_in.dtype}, E_in.device={E_in.device}")

with torch.no_grad():
    E_out = asm(E_in)
    print(rf"E_out.shape={E_out.shape}, E_out.dtype={E_out.dtype}, E_out.device={E_out.device}")


# å¸¦æœ‰è®°å½•çš„æ¨¡æ‹Ÿé€€ç«ï¼Œlogs æ„ä¹‰æ˜¯è®°å½•ã€è®°å½•ä¼˜åŒ–è¿­ä»£çš„è¿‡ç¨‹ä¸­çš„å„ç§å‚æ•°ï¼Œå°¤å…¶æ˜¯å°† spinsÂ±1 æ„å‹åŒæ­¥è®°å½•ï¼Œåç»­å¯ä»¥ç®—è‡ªç›¸å…³
# snapshots_pm1_flatï¼šæ¯éš”å‡ æ­¥ä¿å­˜ä¸€æ¬¡æ•´å¹…è‡ªæ—‹ï¼ˆÂ±1ï¼‰ï¼Œå½¢çŠ¶ [M,S]ï¼Œç”¨äºæ—¶åºç»Ÿè®¡ã€‚
# åœ¨æ—¶é—´ç»´åº¦ä¸Šä¿å­˜å¤šå°‘ç³»ç»ŸçŠ¶æ€ï¼ˆå¿«ç…§ï¼‰: è¾“å…¥å‚æ•° snapshot_every â€”â€” æ§åˆ¶ã€Œè®°å½•é¢‘ç‡ã€
# è‹¥æƒ³è¦è¿ç»­çš„è‡ªç›¸å…³æ›²çº¿ï¼ˆC(t) å¾ˆå¹³æ»‘ï¼‰ï¼Œåº”å½“ä¿å­˜å¾—æ›´å¯†é›†ã€‚
@torch.no_grad()
def simulated_annealing_optimize_with_logs(
        initial_spins_state: torch.Tensor,  # [S] in {0,1}
        asm,  # BandlimitedASM
        input_amp_with_dpm_phase_exp,  # [r,H,W] complex
        weights: Union[np.ndarray, torch.Tensor],  # [r]
        pixel_repeat: int,
        steps: int = 2000,
        T0: float = 1.0,
        alpha: float = 0.999,
        win: Union[int, Tuple[int, int]] = 10,
        device: Union[str, torch.device] = "cuda",
        verbose_every: int = 100,
        snapshot_every: int = 20  # store spin snapshots periodically snapshot_every â€”â€” æ§åˆ¶ã€Œè®°å½•é¢‘ç‡ã€
):
    """
    Single-spin flips with Metropolis acceptance.
    Returns: spins_best, H_best, trace_H, trace_T, trace_m, snapshots_pm1_flat
    """
    device = torch.device(device)
    spins = initial_spins_state.clone().to(torch.int64).cpu()  # [S] 0/1
    S = int(spins.numel())

    # initial energy
    input_spin_phase = spins_vector_to_block_image(spins,
                                                   pixel_repeat=pixel_repeat,
                                                   return_batch=False).cuda()
    E_in_0 = input_amp_with_dpm_phase_exp * torch.exp(-1j * torch.pi * input_spin_phase)
    E_out_0 = asm(E_in_0)
    H_cur = float(hamiltonian_from_Eout(E_out_0, weights, win=win, device=device).item())

    H_best, spins_best = H_cur, spins.clone()
    T = float(T0)

    trace_H = [H_cur]
    trace_T = [T]
    trace_m = [mean_magnetization_pm1(spins01_to_pm1(spins))]

    snapshots = [spins01_to_pm1(spins).to(torch.float32)]  # [S]

    for t in range(1, steps + 1):
        k = np.random.randint(0, S)
        old_val = int(spins[k].item())
        spins[k] = 1 - old_val  # flip 0â†”1

        input_spin_phase = spins_vector_to_block_image(spins,
                                                       pixel_repeat=pixel_repeat,
                                                       return_batch=False).cuda()
        E_in = input_amp_with_dpm_phase_exp * torch.exp(-1j * torch.pi * input_spin_phase)
        E_out = asm(E_in)
        H_new = float(hamiltonian_from_Eout(E_out, weights, win=win, device=device).item())

        dE = H_new - H_cur
        accept = (dE <= 0.0) or (np.random.rand() < np.exp(-dE / max(T, 1e-12)))
        if accept:
            H_cur = H_new
            if H_new < H_best:
                H_best = H_new
                spins_best = spins.clone()
        else:
            spins[k] = old_val  # rollback

        T *= alpha

        if (t % verbose_every) == 0:
            print(f"[SA] step={t:6d}  T={T:.4e}  H_cur={H_cur:.6e}  H_best={H_best:.6e}")

        trace_H.append(H_cur)
        trace_T.append(T)
        trace_m.append(mean_magnetization_pm1(spins01_to_pm1(spins)))

        if (t % snapshot_every) == 0:
            snapshots.append(spins01_to_pm1(spins).to(torch.float32))

    if len(snapshots) == 0:
        snapshots.append(spins01_to_pm1(spins).to(torch.float32))

    snapshots_pm1_flat = torch.stack(snapshots, dim=0).contiguous()  # [M, S]
    return spins_best, H_best, trace_H, trace_T, trace_m, snapshots_pm1_flat


# build 0/1 phase images for panel (1) and (2)
init_spin_phase = spins_vector_to_block_image(initial_spins_state, pixel_repeat=1, return_batch=False)


# autocorrelation (keep a moderate max_lag)
# ç»å…¸è‡ªæ—‹/è‡ªæ—‹ç»ç’ƒé‡Œå¸¸ç”¨çš„ æ—¶é—´è‡ªç›¸å…³å¯¹ã€Œæ—¶é—´èµ·ç‚¹ Ï„ã€å–å¹³å‡ï¼ˆä¹Ÿå¯ç†è§£ä¸ºæ—¶é—´å¹³å‡ï¼‰
# Temporal correlation decay / freezing signature
# C(0)=1;C(t) ä¸‹é™è¶Šå¿«ï¼Œè¯´æ˜ç³»ç»Ÿè¶Šå¿«é—å¿˜è¿‡å»ï¼ˆæ··åˆå¥½/æ¢ç´¢å¿«ï¼‰;
# ä¸‹é™å¾ˆæ…¢æˆ–åœ¨éé›¶å€¼qEAå¹³å°åŒ–ï¼Œè¡¨ç¤ºå†»ç»“/ç»ç’ƒåŒ–æˆ–å¼ºæœ‰åºï¼ˆåŠ¨åŠ›å­¦å¾ˆæ…¢ï¼‰ã€‚
# å¯ä»¥å†™å‡º C(t)=q_EA+(1-q_EA)e^{-t/Ï„} ç„¶ååå‘æ‹Ÿåˆå¾—åˆ°æ¾å¼›æ—¶é—´ Ï„ ä»¥åŠEdwardsâ€“Anderson å‚é‡ğ‘_ğ¸ğ´
# ä¸¥æ ¼æ„ä¹‰ä¸Šè‡ªç›¸å…³æœ€å¥½åœ¨å›ºå®šæ¸©åº¦é˜¶æ®µæµ‹;å›ºå®šæ¸©åº¦æ®µæˆ–è€…æ™šæœŸ/æ—©æœŸï¼ˆæ¸©åº¦å˜åŒ–ä¸å‰§çƒˆçš„æ—¶å€™ï¼‰
# snapshots_pm1_flat:æœŸæœ›è¾“å…¥ snapshots_pm1_flat å½¢çŠ¶ [M, S]ï¼ŒM æ˜¯ä¿å­˜çš„å¿«ç…§æ•°ã€S æ˜¯è‡ªæ—‹æ•°ï¼›æ•°å€¼é¡»ä¸º Â±1ã€‚
# C_t = spin_autocorr(
#     snapshots_pm1_flat,
#     max_lag=min(300, snapshots_pm1_flat.shape[0] - 1),
#     stride_tau=1
# )
# è®¡ç®— C(t) = (1/N) Î£_i(i æŒ‡çš„æ˜¯å¯¹æ‰€æœ‰è‡ªæ—‹)âŸ¨s_i(Ï„)*s_i(Ï„+t)âŸ©_Ï„
# Ï„ æŒ‡çš„æ˜¯é€‰ä¸­æ‰€æœ‰é—´éš”æ—¶é—´å»¶è¿Ÿï¼ˆlagï¼‰ä¸º t çš„è‡ªæ—‹å¯¹åšç‚¹ç§¯
# max_lag å°±æ˜¯ä½ æ‰“ç®—ç®—åˆ°çš„æœ€å¤§å»¶è¿Ÿï¼Œlag æŒ‡çš„æ˜¯ç›¸å¯¹çš„æ—¶é—´å»¶è¿Ÿ
# max_lag ä¸èƒ½è®¾ç½®çš„å¤ªå¤§ï¼Œå¦‚æœè®¾å¤ªå¤§ï¼ŒååŠæ®µå‡ ä¹æ²¡æœ‰æœ‰æ•ˆå¹³å‡ï¼ˆå› ä¸º Ï„+t è¶…å‡ºèŒƒå›´ï¼‰ï¼Œæ•°å€¼ä¼šéå¸¸å™ªã€‚
# è§„å¾‹ï¼šå° max_lag â†’ å¿«é€Ÿè®¡ç®—ï¼Œæ›²çº¿çŸ­ä½†å¹³æ»‘ï¼›
# å¤§ max_lag â†’ å¯ä»¥çœ‹åˆ°é•¿æ—¶é—´è¡°å‡ï¼ˆä¾‹å¦‚æ˜¯å¦æœ‰ Edwardsâ€“Anderson å¹³å°ï¼‰ï¼Œä½†æ›²çº¿æœ«å°¾ä¼šå˜å™ªã€‚
# ä¸€èˆ¬å–max_lag â‰ˆ M / 10 æˆ– M / 5ï¼ˆM æ˜¯å¿«ç…§æ€»æ•°ï¼‰ è¿™é‡Œ M = snapshots_pm1_flat.shape[0]
# stride_tau â€”â€” æ§åˆ¶ã€Œæ—¶é—´å¹³å‡çš„ä¸‹é‡‡æ ·é—´éš”ã€;stride_tau=1 â†’ æ¯ä¸ªå¿«ç…§éƒ½å‚ä¸ï¼›æé«˜ç»Ÿè®¡ç‹¬ç«‹æ€§ï¼›ä¸æ”¹å˜è‡ªç›¸å…³å½¢çŠ¶ï¼ˆåªæ˜¯æ›´ç¨€ç–é‡‡æ ·ï¼‰ã€‚
# åœ¨ t ç¡®å®šåï¼ŒâŸ¨s_i(Ï„)*s_i(Ï„+t)âŸ©_Ï„ å…¶å®æ˜¯é’ˆå¯¹ M-t ä¸ªæ—¶é—´çª—å£æ¥ç®—ï¼ŒæŒ‰ç†è¯´ä¼šæœ‰ (M-t)*(M-t-1)/2 pairs, ä½†æ˜¯å¯¹ Ï„ å†æ¬¡ä¸‹é‡‡æ ·ï¼Œå¹¶éå¯¹æ‰€æœ‰ M-t ä¸ªæ—¶é—´çª—å£éƒ½ä¸¤ä¸¤è®¡ç®—


def exp_decay(t, qEA, tau):
    return qEA + (1 - qEA) * np.exp(-t / tau)


def fit_autocorr(t, C):
    try:
        popt, _ = curve_fit(exp_decay, t, C, bounds=([0, 0], [1, np.inf]))
        return popt
    except:
        return np.nan, np.nan


with torch.no_grad():
    for T in T_list:
        spins_best, H_best, trace_H, trace_T, trace_m, snapshots_pm1_flat = simulated_annealing_optimize_with_logs(
            initial_spins_state=initial_spins_state,
            asm=asm,
            input_amp_with_dpm_phase_exp=input_amp_with_dpm_phase_exp,
            weights=weights,
            pixel_repeat=pixel_repeat_Coupling_Jij,
            steps=iterations,
            T0=T,
            alpha=alpha,
            win=collector,
            device=device,
            verbose_every=200,
            snapshot_every=1
        )
        snapshots = snapshots_pm1_flat
        # t = np.arange(snapshots.shape[0] // 5 + 1)
        # 1. å…ˆç¡®å®š max_lagï¼ˆæƒ³ç”¨æ»¡çª—å£å°±è®¾ M-1ï¼‰
        max_lag = int(snapshots.shape[0] // 5 - 1)  # 0...M-1 â†’ é•¿åº¦ M # æ¯•ç«Ÿæ˜¯ä» C(0) åˆ° C(max_lag)
        # å› æ­¤ snapshots.shape[0] ä¸ªçª—å£ï¼Œç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªçš„é—´éš”æœ€å¤šä¹Ÿå°± ï¼ˆsnapshots.shape[0] - 1ï¼‰
        t = np.arange(max_lag + 1)  # t åªæ˜¯åˆ»åº¦å‘é‡ï¼Œä¸ C_t ä¸€ä¸€å¯¹åº”ï¼Œæ–¹ä¾¿ç”»å›¾å’Œæ‹Ÿåˆï¼›å®ƒä¸å½±å“è®¡ç®—ï¼Œåªå½±å“æ¨ªè½´æ ‡ç­¾ã€‚

        C_t = spin_autocorr_general(
            snapshots,
            # max_lag=min(300, snapshots_pm1_flat.shape[0] - 1), # å¦‚æœ max_lag = 300ï¼Œä½ åªè®¡ç®—ä» t=0 åˆ° t=300ï¼›
            max_lag=max_lag,
            stride_tau=1  # å¯¹Ï„çš„ä¸‹é‡‡æ ·æ­¥é•¿ã€‚>1 å¯é™å™ªã€é™ç®—é‡ï¼›=1 è¡¨ç¤ºç”¨æ»¡å…¨éƒ¨ ğœ
        )

        C_dict[float(T)] = (t, C_t)
        qEA, tau = fit_autocorr(t, C_t)
        print(rf"T={T}, qEA={qEA}, tau={tau}")
        # qEA â†’ å¹³å°é«˜åº¦ï¼ˆEdwardsâ€“Andersonåºå‚é‡ï¼‰tau â†’ æ¾å¼›æ—¶é—´ï¼ˆæŒ‡æ•°è¡°å‡ç‰¹å¾æ—¶é—´ï¼‰
        # å¦‚æœC(t)å‡ ä¹ä¸ä¸‹é™ï¼ˆä½æ¸©ç»ç’ƒï¼‰ï¼Œæ‹Ÿåˆä¼šæŠŠè¡°å‡éƒ¨åˆ†å½’å› äºå¾ˆå¤§çš„tau â†’ æ•°å€¼ä¸Štau â‰« æ¨¡æ‹Ÿçª—å£ï¼Œè¡¨ç¤ºç³»ç»Ÿææ…¢åœ°é—å¿˜åˆå§‹æ„å‹ã€‚
        qEA_arr.append(qEA)
        tau_arr.append(tau)

# 1. ç¡®ä¿æ¸©åº¦ä»å°åˆ°å¤§ â†’ å›¾ä¾‹è‡ªä¸Šè€Œä¸‹ = é«˜æ¸©â†’ä½æ¸©
T_list_sorted, C_items = zip(*sorted(C_dict.items(), key=lambda x: x[0]))

plt.figure(figsize=(6, 4))
for T, (t, C) in zip(T_list_sorted, C_items):
    # ---- æ•£ç‚¹ ----
    plt.semilogx(t, C, 'o', ms=3, label=f'T = {T:.2f}')

    # ---- æ‹Ÿåˆçº¿ ----
    qEA, tau = fit_autocorr(t, C)
    if not (np.isnan(qEA) or np.isnan(tau)):
        plt.semilogx(t, exp_decay(t, qEA, tau), '-', lw=1,
                     color=plt.gca().lines[-1].get_color(), alpha=0.8)

plt.xlabel('Monte-Carlo time t')
plt.ylabel('C(t)')
plt.ylim(-0.05, 1.05)
plt.grid(True, which='both', ls='--', alpha=.4)
plt.legend(ncol=2, fontsize=8, frameon=False)
plt.title('Autocorrelation vs time @ different T')
plt.tight_layout()
plt.show()
